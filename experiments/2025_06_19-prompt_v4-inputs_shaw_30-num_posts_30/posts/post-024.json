{
  "input": "Share video: multimodal LLMs. Using Llama 3.2 Vision to do CV",
  "writing_steps": "Step 1: Purpose & Audience\n- Purpose: Share a resource (video) and lightly educate about a trending technical frontier: multimodal LLMs; validate interest in computer vision applications using new open weights models.\n- Audience: AI engineers, developers, practitioners curious about LLMs applied to computer vision tasks, followers keeping up with Shaw’s project/tool stack.\n\nStep 2: Wireframe\n- Hook: Grounded in a factual observation about the rapid evolution in multimodal LLMs (text and vision), and the growing accessibility via open weights (Llama 3.2 Vision).\n- Body: Discuss how Llama 3.2 Vision enables computer vision workflows, what’s possible now, and link the video for those who want practical demos or more technical walkthroughs.\n- CTA: Ask for practical tips, favorite use cases, or what people want to see next with CV + open weights LLMs.\n\nStep 3: Body/Meat\nMultimodal LLMs (text + vision in one model) have gotten a lot more accessible lately.\nLlama 3.2 Vision now lets you build computer vision pipelines using open weights—no more waiting for closed APIs.\n\nSome example use cases:\n- Image captioning\n- Visual QA (ask questions about images)\n- Simple object detection/localization\n\nIf you're experimenting with Llama 3.2 Vision, you can create and test computer vision workflows end-to-end with local tools, or integrate into an agent pipeline.\n\nI walk through some first steps and setups here:\n[video link]\n\nStep 4: CTA\nWhat workflows or use cases are you building with multimodal LLMs?\n\nStep 5A: Hook Directions\n1. [Trend/urgency]: \"Open-source LLMs can now see—here’s what you can build.\"\n2. [Beginner-to-expert clarity]: \"Multimodal models aren’t just for research anymore.\"\n3. [Problem/solution]: \"Want to run Vision models outside of closed APIs?\"\n4. [User’s own phrasing]: \"Using Llama 3.2 Vision for computer vision.\"\n5. [Results/outcomes]: \"What I learned running computer vision tasks with Llama 3.2 Vision.\"\n\nStep 5B: Short notes:\n1. Open-sourcing, open weights; vision tasks; Llama 3.2 Vision, practical/real projects.\n2. Lowering the barrier—developers can now use vision pipelines locally.\n3. Address people frustrated with API limits/costs.\n4. Direct and straightforward, but not the most grabbing.\n5. Teasing concrete outcomes from trying these tools.\n\nStep 5C: Choose strongest direction & write final hook\nChoose trend/urgency: \"Open-source LLMs can now see—here’s what you can build with Llama 3.2 Vision.\"\n\nStep 6: Draft full post\nOpen-source LLMs can now see—here’s what you can build with Llama 3.2 Vision.\n\nMultimodal LLMs (text + vision) aren’t just for big labs anymore. With Llama 3.2 Vision, you can run computer vision workflows using open weights, test new ideas locally, and skip closed APIs.\n\nSome things you can try right now:\n- Image captioning\n- Visual question answering (VQA)\n- Object localization (basic detection)\n\nI put together a quick video to demo setup, first steps, and practical possibilities:\n[video link]\n\nWhat workflows or use cases are you building with multimodal LLMs?\n\nStep 7: Review\n- No business clichés, idioms, or motivational statements.\n- Factual, practical, and focuses on a specific tool (Llama 3.2 Vision).\n- CTA is single, specific, and geared to practical/technical discussion.\n- Visual scannability with bullet points.\n- Hook is curiosity/trend-driven, not rhetorical or personal-story based.\n- Voice aligns with provided examples: clear, approachable, direct, grounded.\n\nStep 8: Final Post",
  "final_post": "Open-source LLMs can now see—here’s what you can build with Llama 3.2 Vision.\n\nMultimodal LLMs (text + vision) aren’t just for research labs anymore. With Llama 3.2 Vision, you can run computer vision workflows using open weights and skip closed APIs entirely.\n\nSome things you can try right now:\n- Image captioning\n- Visual question answering (VQA)\n- Basic object localization\n\nI put together a quick video walkthrough: setup, first steps, and a look at what’s possible.\n\n[video link]\n\nWhat workflows or use cases are you building with multimodal LLMs?"
}