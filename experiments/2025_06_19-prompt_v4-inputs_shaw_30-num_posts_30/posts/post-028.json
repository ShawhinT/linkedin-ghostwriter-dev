{
  "input": "Share blog: multimodal models. 3 ways to make LLMs multimodal.\nLLM + Tools\nLLM + Adapters\nUnified Models",
  "writing_steps": "Step 1: Purpose and Audience\n- Purpose: Educate readers about multimodal models in AI, specifically three ways to make LLMs multimodal, and share a blog post for deeper exploration.\n- Audience: AI engineers, researchers, technical founders, and anyone interested in advancing their understanding of LLM architectures and capabilities.\n\nStep 2: Wireframe\n- Hook: Bold, curiosity-driven statement about LLMs and multimodality\n- Body: Briefly introduce the increasing importance of multimodal models. List the three main approaches (LLM+Tools, LLM+Adapters, Unified Models) with a sentence on each.\n- Link to blog for full breakdown\n- CTA: Ask what approaches or integrations people find most promising in their own work\n\nStep 3: Body (Meat)\nMultimodal models—AI systems that process text, images, audio, and more—are quickly becoming foundational in research and real-world apps.\n\nHere are three practical ways researchers and builders are making LLMs multimodal:\n\n1. LLM + Tools\n   - The base LLM is paired with external tools (like image or audio processors). The LLM routes requests to the right tool depending on the input or the user's question.\n\n2. LLM + Adapters\n   - Lightweight adapter modules plug into LLMs, allowing them to handle new input types (images, audio, video) without training an entirely new unified model. Efficient and modular.\n\n3. Unified Multimodal Models\n   - A single model (like GPT-4V, Gemini, or Flamingo) trained end-to-end on multiple data types. All-in-one, but requires huge amounts of multimodal data and compute.\n\nStep 4: CTA/Question\nWhich approach or integration would you add or find most practical in your own work?\n\nStep 5: Hook Exploration\nA. [Beginner-to-expert clarity] “Everyone’s talking about LLMs going multimodal—but how does it actually work?”\nB. [Problem/solution] “Want to use LLMs for image, audio, or video tasks? Here are 3 real ways to make it happen.”\nC. [Trend/urgency] “2024 is the year LLMs go multimodal. Here’s how researchers are doing it.”\nD. [User’s phrasing] “3 ways to make LLMs multimodal.”\n\n- Most promising: Direction B, as it teases practical approaches that tie in with the rest of the post.\n\nStep 6: Draft Post\nWant to use LLMs for image, audio, or video tasks? Here are 3 real ways to make it happen.\n\nMultimodal models are shifting how we build and use AI. Whether you’re experimenting or deploying in production, there are a few main paths:\n\n1. LLM + Tools\nThe LLM passes requests to external tools specialized for different data types—think plug-and-play for images, audio, or video.\n\n2. LLM + Adapters\nAdapters let you connect new modalities to your LLM without retraining the whole model. They’re efficient and modular.\n\n3. Unified Multimodal Models\nThese are single models trained from scratch on text and non-text data together (like GPT-4V) but require lots of resources.\n\nI break down each approach in detail in my latest blog post: [link]\n\nWhich approach or integration would you add or find most practical in your own work?\n\nStep 7: Review for Style, Voice, and Flow\n- No business clichés, idioms, or motivational language.\n- Clear, modular bullet points for scannability.\n- Approachable tone with direct invitation to add technical insights.\n- Matches Shaw’s style from example posts.\n\nStep 8: Final Post",
  "final_post": "Want to use LLMs for image, audio, or video tasks? Here are 3 real ways to make it happen:\n\nMultimodal models—systems that handle more than just text—are quickly shaping the next generation of AI projects. Here are three ways people are actually making LLMs multimodal today:\n\n1. LLM + Tools\nThe LLM routes parts of a user’s request to external tools specialized for images, audio, or video. This lets you combine best-in-class models for each modality (without retraining everything).\n\n2. LLM + Adapters\nAdapters are lightweight modules that plug into the LLM to process additional input types. They’re efficient and modular—so you can add new capabilities without starting from scratch.\n\n3. Unified Multimodal Models\nA single model pre-trained on multiple data types (like GPT-4V or Gemini) can handle text, images, and more in one system. These need a lot of data and compute, but give you a fully unified interface.\n\nI break down all three approaches in my latest blog post: [link]\n\nWhich approach or integration would you add or find most practical in your own work?"
}