{
  "input": "Share blog: multimodal models. 3 ways to make LLMs multimodal.\nLLM + Tools\nLLM + Adapters\nUnified Models",
  "writing_steps": "Step 1:\nPurpose: Educate (share 3 ways to make LLMs multimodal) and promote a deeper blog post.\nTarget Audience: AI engineers, technical product builders, and those curious about practical AI/ML techniques.\n\nStep 2: Post wireframe\n- Hook: Short, bold opener about multimodal LLMs and why they matter\n- Body: Outline 3 ways to make LLMs multimodal (LLM+Tools, LLM+Adapters, Unified Models) in clear, skimmable format\n- Link: Point to the blog for details\n- CTA: Ask for reader experience, e.g., 'Which approach have you tried?'\n\nStep 3: Body\nMaking LLMs multimodal (able to process text, images, audio, and more) is a core challenge in AI today. Here are 3 main strategies:\n\n1. LLM + Tools: Use external APIs or separate models (like image classifiers) alongside your LLM. Example: Pass images to a vision API, then feed results to your LLM for reasoning.\n\n2. LLM + Adapters: Add dedicated modules (\"adapters\") to the LLM architecture, allowing it to handle new data types without retraining the whole model.\n\n3. Unified Models: Train a single model from scratch to natively handle text, images, audio, etc. Most complex, but highest potential.\n\nSee the blog for a deeper breakdown and use-case examples.\n\nStep 4: CTA\nWhich of these have you worked with?\n\nStep 5: Hook Directions\nA. [Problem/solution] “Most LLMs can only work with text—here’s how to make them multimodal”\nB. [Beginner-to-expert clarity] “Confused about multimodal AI? 3 ways to make your LLMs see, hear, and do more”\nC. [Trend/urgency] “Multimodal models are the future of AI—these are the methods powering the most advanced systems”\nD. [User’s own phrasing] “3 ways to make LLMs multimodal”\nE. [Action-oriented] “How to make your LLM project multimodal in 2024”\n\nStep 5B: Notes\nA. Highlight text-only limitations, offer solutions\nB. Address confusion, demystify options\nC. Frame as a must-know for future-proofing skills\nD. Straightforward, listicle/educational\nE. Helpful, immediately actionable for builders\n\nStep 5C: Pick B. Clarity: “Confused about multimodal AI? 3 ways to make your LLMs see, hear, and do more”\n\nStep 6: Draft post\nConfused about multimodal AI? 3 ways to make your LLMs see, hear, and do more.\n\nMaking large language models multimodal (able to process text, images, audio, etc.) is now a key engineering challenge.\n\nHere are 3 main ways to tackle it:\n\n1. LLM + Tools: Use external APIs or specialist models for images/audio, then send their outputs to your LLM.\n\n2. LLM + Adapters: Add adapter modules to your LLM with minimal retraining, so it can process new data types directly.\n\n3. Unified Models: Train a new model from scratch to handle multiple data types natively—the hardest, but the most powerful approach.\n\nI just shared a deeper breakdown and practical use cases in my new blog post:\n[link]\n\nWhich of these approaches have you tried in your projects?\n\nStep 7: Review for Shaw’s style and checklist\n- Clear language, scannable list format\n- No more than one em dash\n- CTA is a single, direct question\n- Minimal jargon, approachable tone\n\nStep 8: Final post",
  "final_post": "Confused about multimodal AI? 3 ways to make your LLMs see, hear, and do more.\n\nMaking large language models multimodal (able to process text, images, audio, etc.) is now a key engineering challenge. Here are the 3 main strategies:\n\n1. **LLM + Tools**\nPair your language model with external APIs or specialist models (like vision or audio classifiers), then feed their outputs into your LLM for further reasoning.\n\n2. **LLM + Adapters**\nAdd modular adapters to your LLM so it can process new data types—without retraining the full model.\n\n3. **Unified Models**\nTrain a single, end-to-end model to handle multiple data types natively. This is the most complex, but also the most powerful.\n\nI just shared a blog post breaking down these frameworks (with practical examples and pros/cons):\n\n[link]\n\nWhich of these approaches have you tried in your own AI projects?"
}