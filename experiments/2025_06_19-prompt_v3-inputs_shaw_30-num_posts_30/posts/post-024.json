{
  "input": "Share video: multimodal LLMs. Using Llama 3.2 Vision to do CV",
  "writing_steps": "Step 1: Purpose & Target Audience\n- What is the purpose of this post? Share informative content (video) about using Llama 3.2 Vision for computer vision (CV) tasks, and spark discussion/interest around multimodal LLMs.\n- Who is the target audience? AI engineers, technical practitioners, and those interested in emerging LLM capabilities (esp. multimodal), likely Shaw's LinkedIn audience of AI learners and builders.\n\nStep 2: Post Wireframe\n- Hook: Introduce the potential of new multimodal LLMs (vision + language), specifically highlighting something interesting about Llama 3.2 Vision + CV use cases.\n- Meat: Share the video, explain briefly what it shows (Llama 3.2 Vision doing CV), and frame why this matters or is interesting (new capabilities, practical implications for those building with LLMs, etc.)\n- CTA: Ask the audience how they see vision+language LLMs changing their workflow (specific to engineers, researchers, etc.)\n\nStep 3: Write the Meat\nI just made a quick video showing Llama 3.2 Vision in action for computer vision tasks. It’s impressive to see how these models are bridging the gap between traditional NLP and CV problems—using a single model for both text and visual input.\n\nThis opens up interesting possibilities for prototyping and experimenting with multimodal projects—without having to juggle multiple specialized models or frameworks.\n\nLink to video: [insert video link]\n\nStep 4: CTA\nHow do you see vision + language LLMs changing your workflow?\n\nStep 5: Write the Hook\nStep 5A: Brainstorm labeled directions:\n- [User's own phrasing/announcement] \"Sharing a quick demo: Llama 3.2 Vision on CV tasks\"\n- [Trend/urgency] Multimodal LLMs are now mainstream (vision + text in one package)\n- [Problem/solution] Using a single model for both NLP and CV—less complexity, more flexibility\n- [Beginner-to-expert clarity] What can you actually do with these new vision-enabled LLMs?\n\nStep 5B: Notes for each direction:\n- [User's phrasing] straightforward, but lacks framing\n- [Trend/urgency] emphasize shift from siloed ML models to unified LLMs\n- [Problem/solution] appeals to engineers who hate gluing models together\n- [Beginner-to-expert] practical applications, “what’s new/possible?”\n\nStep 5C: Choose the strongest direction: [Trend/urgency + practical clarity]—People may not have seen these LLMs actually solving real CV tasks—highlight what’s possible now and why it matters. \n\nDraft hook: Multimodal LLMs aren’t just hype—here’s how you can use Llama 3.2 Vision for real computer vision problems today.\n\nStep 6: Draft Full Post\nMultimodal LLMs aren’t just hype—here’s how you can use Llama 3.2 Vision for real computer vision problems today.\n\nI just made a quick video showing Llama 3.2 Vision in action for CV tasks. It’s impressive to see how these models are bridging the gap between traditional NLP and computer vision—allowing you to work with text and images in one shot.\n\nThis opens up new possibilities for rapid prototyping and building multimodal projects—without needing a stack of different ML models for every task.\n\nWatch the demo here: [insert video link]\n\nHow do you see vision + language LLMs changing your workflow?\n\nStep 7: Review\n- No rhetorical/motivational openers\n- Clear, scannable, factual\n- One em dash in first line—acceptable if for emphasis, but could also be replaced with a colon or period to match Shaw’s style more closely\n- Factual, invites discussion, shares a resource, clear and action-oriented\n\nStep 8: Final Post",
  "final_post": "Multimodal LLMs aren’t just hype: here’s how you can use Llama 3.2 Vision for real computer vision problems today.\n\nI just recorded a short video showing Llama 3.2 Vision handling CV tasks. It’s impressive to see how these models are bridging the gap between traditional NLP and computer vision—all with a single model that takes both text and images as input.\n\nThis makes it easier to prototype and experiment with multimodal projects, without the extra overhead of integrating multiple specialized tools.\n\nWatch the demo here: [insert video link]\n\nHow do you see vision + language LLMs changing your workflow?"
}