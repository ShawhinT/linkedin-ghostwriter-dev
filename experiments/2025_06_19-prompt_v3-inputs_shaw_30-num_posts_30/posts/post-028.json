{
  "input": "Share blog: multimodal models. 3 ways to make LLMs multimodal.\nLLM + Tools\nLLM + Adapters\nUnified Models",
  "writing_steps": "Step 1: Purpose and Audience\n- Purpose: Educate about the ways to make large language models (LLMs) multimodal and share a blog post on this topic.\n- Audience: AI practitioners, engineers interested in LLMs, and anyone curious about deep learning advances.\n\nStep 2: Wireframe\n- Hook\n- The meat (3 approaches to multimodal LLMs: LLM + Tools, LLM + Adapters, Unified Models)\n- Link to the blog\n- Focused question\n\nStep 3: Body\nMaking LLMs multimodal is one of the most exciting frontiers in AI right now. There are three core approaches I've seen:\n\n1. LLM + Tools: Give your LLM access to external tools that handle images, audio, etc. The LLM remains text-focused but can call APIs for other modalities.\n2. LLM + Adapters: Use adapters (small neural modules) that let the LLM process other data types by bridging between modalities.\n3. Unified Models: Train a single neural network that natively understands and generates multiple data types (text, images, and more).\n\nEach approach comes with its own tradeoffs—around flexibility, performance, and complexity.\n\nStep 4: CTA\n\"Curious how folks are approaching this. Which path are you most excited about?\"\n\nStep 5A: Hook Directions\n- [Trend/urgency] Multimodal is “the next big thing” in LLMs\n- [Problem/solution] “Most people think you need to retrain from scratch to go multimodal—here are easier ways.”\n- [User’s phrasing] Announcement about blog sharing\n- [Beginner-to-expert clarity] “There are just three ways to make LLMs multimodal. Here they are.”\n- [Personal observation] “After exploring multimodal models, these are the three practical directions I see.”\n\nStep 5B: Notes on Each\n- Trend/urgency: Multimodal models are moving fast\n- Problem/solution: Emphasize practical vs. complex options\n- User’s phrasing: “Sharing blog: multimodal models... 3 ways to make LLMs multimodal.”\n- Beginner-to-expert: Lay out the landscape simply\n- Personal: Honest technical framing\n\nStep 5C: Strongest Hook\n“Making LLMs multimodal comes down to 3 main approaches—each with unique tradeoffs.”\n\nStep 6: Full Draft\nMaking LLMs multimodal comes down to 3 main approaches—each with unique tradeoffs.\n\n1. LLM + Tools: Give the model access to external tools (like image or audio APIs). Keeps your LLM simple, but can feel modular and less seamless.\n2. LLM + Adapters: Plug in adapters to bridge from images/audio to text. Flexible and easier to update, but performance can depend on adapter quality.\n3. Unified Models: Train a single network to handle text, images, (and sometimes even audio) natively. This is powerful, but data and compute requirements go way up.\n\nI just published a blog breaking these paths down in more detail:\n[Link to blog]\n\nWhich approach are you most excited about experimenting with?\n\nStep 7: Style, Voice & Flow\n- Kept the tone factual, clear, and approachable\n- Scannability through bullet points\n- No overhype or imperatives\n- Only one em dash\n- Ends with a single, focused question\n\nStep 8: Final Post",
  "final_post": "Making LLMs multimodal comes down to 3 main approaches—each with unique tradeoffs.\n\n1. LLM + Tools: Give the model access to external tools (like image or audio APIs). The LLM stays text-focused, but can call out to specialized handlers for other data types.\n2. LLM + Adapters: Plug adapters between the LLM and new inputs/outputs (like images or audio). Flexible for updates, but your results depend on adapter quality.\n3. Unified Models: Train a single network that natively consumes and generates multiple types of data. Highest potential—but also highest complexity and data needs.\n\nI just shared a blog post breaking down these paths in more detail:\n[Link to blog]\n\nWhich approach are you most interested in experimenting with?"
}