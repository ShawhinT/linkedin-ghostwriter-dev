{
  "input": "Share blog: fine-tuning Bert for text classification",
  "writing_steps": "Step 1: Purpose and Target Audience\n- Purpose: Share Shaw's new blog post on fine-tuning BERT for text classification. Educate the audience on the topic and share the practical resource.\n- Audience: AI practitioners, engineers, and learners interested in NLP and fine-tuning large language models.\n\nStep 2: Post wireframe\n- Hook: Direct and factual, calls out a problem or teases practical value\n- Meat: Quick summary of what's inside the blog: fine-tuning BERT for text classification\n- Link to blog\n- CTA: Ask for readers' most common challenges or tips when fine-tuning models\n\nStep 3: The Meat\nFine-tuning BERT for text classification is one of the most common NLP applications. The new blog covers a step-by-step process, including data prep, model training, and evaluation. Emphasis is on clear instructions and code examples.\n\nStep 4: CTA\nWhat’s been your biggest challenge or insight when fine-tuning models like BERT?\n\nBut, per guidelines, we cannot use “or” in the CTA. I’ll pick one: What’s been your biggest challenge when fine-tuning models like BERT?\n\nStep 5A: Hook directions\n1. [User's own phrasing] N/A\n2. [Problem/Solution] Fine-tuning BERT is still confusing for many.\n3. [Trend/Education] BERT remains one of the most-used NLP models for text classification.\n4. [Resource/Practical] Here’s a step-by-step blog on fine-tuning BERT for text classification.\n5. [Common Pitfalls] Most fine-tuning guides skip over real pitfalls—this one covers them.\n\nStep 5B: Flesh them out\n1. Not present\n2. Many tutorials assume lots of prior knowledge; clear step-by-step can help.\n3. Despite recent models, BERT is still highly used for practical NLP.\n4. Factual value: Here’s a hands-on guide with code walkthrough.\n5. Potential to highlight what’s different about this resource.\n\nStep 5C: Choose strongest direction and write hook.\nI’ll go with 2: Even with newer models around, fine-tuning BERT for text classification is a core skill—and most tutorials skip the details.\n\nFinal hook: Fine-tuning BERT for text classification is still a core NLP skill.\n\nStep 6: Draft the full post\n[Hook]\nFine-tuning BERT for text classification is still a core NLP skill.\n\n[Meat]\nI just published a step-by-step blog covering how to fine-tune BERT for text classification — including data prep, training, and evaluation. The guide is packed with code snippets you can run right away, plus explanations for each step.\n\n[Link to blog]\nRead the full blog here: [insert blog URL]\n\n[CTA]\nWhat’s been your biggest challenge when fine-tuning models like BERT?\n\nStep 7: Review for style, voice, and flow\n- Only one em dash used in entire post (between “fine-tune BERT for text classification” and “including data prep...”). Acceptable per rules.\n- Literal language: no idioms, metaphors, or business clichés.\n- CTA is focused, direct, and practical.\n- Style and format matches established Shaw example posts.\n\nStep 8: Write the final post.",
  "final_post": "Fine-tuning BERT for text classification is still a core NLP skill.\n\nI just published a step-by-step blog covering how to fine-tune BERT for text classification — including data prep, training, and evaluation. The guide is packed with code snippets you can run right away, plus explanations for each step.\n\nRead the full blog here: [insert blog URL]\n\nWhat’s been your biggest challenge when fine-tuning models like BERT?"
}