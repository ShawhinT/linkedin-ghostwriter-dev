{
  "input": "Share blog: multimodal embeddings",
  "writing_steps": "Step 1: Purpose and Audience\n- Purpose: Share Shaw's new blog post on multimodal embeddings and educate followers about what they are and why they matter.\n- Audience: AI engineers, researchers, and technically curious followers interested in the latest AI approaches (especially those wanting clarity on new terms).\n\nStep 2: Wireframe\n- Hook: Pose a factual claim or gap about multimodal embeddings—many have heard the term, but few know how they work or why they matter.\n- Body: Briefly define multimodal embeddings. Explain how they differ from unimodal embeddings. Why this topic is increasingly important (e.g., images + text, video + audio). Link to blog.\n- CTA: Ask readers to share their favorite use case or biggest question about multimodal AI.\n\nStep 3: Body\nA lot of AI progress lately comes from better ways of connecting information from different sources—text + images, sound + video, etc. Multimodal embeddings are a technique for representing these combinations in a way machines can analyze, search, and learn from. \n\nThe blog breaks down:\n- What multimodal embeddings are\n- How they differ from single-modality methods\n- High-level approaches and where they’re useful (like product search, captioning, and recommendation systems)\n\nStep 4: CTA\nWhat’s one use case for multimodal embeddings you find interesting?\n\nStep 5: Hook brainstorm:\n- [Gap/clarity] “Most have heard of multimodal embeddings, but where do they actually matter in practice?”\n- [Problem/solution] “Combining text + images is still confusing for many—here’s a plain explanation.”\n- [Beginner-to-expert] “Multimodal embeddings: why they’re the next wave in AI, explained simply.”\n- [User phrasing] “Share blog: multimodal embeddings”\n- [Trend/urgency] “More systems than ever use multimodal data—here’s what you need to know about embeddings.”\n\nNotes: Top direction is the beginner-to-expert: plain explanation for those who keep hearing the term but want a practical understanding.\n\nStep 6: Draft\nMultimodal embeddings: why they're important for the next wave of AI\n\nA lot of recent AI progress comes from new ways of combining data from text, images, video, and sound.\n\nMultimodal embeddings are a technique for connecting all these types of information in a way machines can analyze, search, and learn from.\n\nI just published a new blog post explaining:\n- What multimodal embeddings are\n- How they differ from unimodal methods\n- Where they're actually useful (like search, captioning, and recommendations)\n\nRead the full post here ➡️ [link]\n\nWhat’s one use case for multimodal embeddings you find interesting?\n\nStep 7: Review\n- Only one em dash (“—”) used in the title (“why they're important for the next wave of AI”).\n- Factual, educational, and practical. No motivational language or business clichés. \n- No summarizing tricolon, no rhetorical opener, direct language throughout. Link is at the top before CTA.\n- Cross-checked with Examples 1, 4, and 5: clear topic, brief context, shares a resource, asks factual practical question.\n\nStep 8: Final Post",
  "final_post": "Multimodal embeddings: why they're important for the next wave of AI\n\nA lot of recent AI progress comes from new ways of combining data from text, images, video, and sound.\n\nMultimodal embeddings are a technique for connecting all these types of information in a way machines can analyze, search, and learn from.\n\nI just published a new blog post breaking down:\n- What multimodal embeddings are\n- How they differ from unimodal methods\n- Where they're actually useful (like search, captioning, and recommendations)\n\nRead the full post here ➡️ [link]\n\nWhat’s one use case for multimodal embeddings you find interesting?"
}