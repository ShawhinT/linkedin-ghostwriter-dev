{
  "input": "Share blog: multimodal models. 3 ways to make LLMs multimodal.\nLLM + Tools\nLLM + Adapters\nUnified Models",
  "writing_steps": "Step 1: Purpose and Audience\n- Purpose: Share a new blog post on multimodal models, and provide practical ways to make LLMs multimodal.\n- Audience: AI/ML engineers, AI learners, and anyone interested in building advanced LLM applications.\n\nStep 2: Post Wireframe\n[Hook teasing 3 practical methods for multimodality]\n[Concrete breakdown of each method (LLM+Tools, LLM+Adapters, Unified Models)]\n[Link to blog]\n[CTA: Which approach are people most interested in?]\n\nStep 3: Body\nMost LLMs are text-only, but many new projects need to process images, audio, or other data types. There are three main ways to make LLMs multimodal:\n\n- LLM + Tools: Combine the LLM with external APIs or plugins to process images or other formats. For example, have the LLM call an OCR tool or a vision API when it receives an image.\n- LLM + Adapters: Add lightweight neural adapters that translate non-text modalities (like images) into a format LLMs can process (such as text embeddings). This is more efficient than retraining the full model.\n- Unified Models: Train one large model that “natively” handles multiple data types—like GPT-4V or Gemini—but this is the most resource-intensive and least customizable approach.\n\nStep 4: CTA\nWhat’s the first multimodal project you want to build?\n\nStep 5: Hook Directions\n1. [Problem/solution] \"Need LLMs to process images or audio? Here are 3 practical approaches.\"\n2. [Beginner-to-expert clarity] “Most people think LLMs are text-only, but here’s how to go multimodal.\"\n3. [User’s own phrasing] “3 ways to make LLMs multimodal.”\n4. [Trend/urgency] “Multimodal is everywhere now—here’s how you can build it.”\n5. [Technical framework] “LLM + Tools, Adapters, Unified Models—what actually works?”\n\nStep 5B: Short Notes\n1. [Problem/solution]: Call out need for images/audio.\n2. [Beginner-to-expert clarity]: Call out misconception.\n3. [User’s phrasing]: Direct.\n4. [Trend]: Timely reference.\n5. [Technical framework]: Use all three method names in hook.\n\nStep 5C: Choose strongest direction.\n- Go with [Problem/solution]: “Need LLMs to process images or audio? Here are 3 practical approaches.”\n\nStep 6: Draft\nNeed LLMs to process images or audio? Here are 3 practical approaches.\n\nMost LLMs are still text-only, but it’s possible to make your language model apps multimodal using three frameworks:\n\n- LLM + Tools: Connect your LLM to external APIs for image, audio, or video processing. For example, use an OCR tool when the LLM receives an image.\n- LLM + Adapters: Plug in neural adapters to translate images or audio into text or embeddings the LLM can use. This keeps everything lightweight and modular.\n- Unified Models: Use or train a single model that natively handles multiple data types, like GPT-4V or Gemini. This is the most powerful but requires the most resources.\n\nWrote up a blog breaking down each method with practical use cases and code examples. \n(Link)\n\nWhat’s the first multimodal project you want to build?\n\nStep 7: Review\n- No business clichés, no motivational content, only one em dash (if any), clear technical focus, CTA is single and practical, structure is clear.\n\nStep 8: Final Post",
  "final_post": "Need LLMs to process images or audio? Here are 3 practical approaches.\n\nMost LLMs are still text-only, but there are three frameworks you can use to build multimodal projects:\n\n- LLM + Tools: Connect your LLM to external APIs for image, audio, or video processing. For example, you can have the LLM call an OCR tool whenever it receives an image.\n- LLM + Adapters: Use neural adapters to translate images or other non-text inputs into text or embeddings the LLM can understand. This approach is lightweight and modular.\n- Unified Models: Use a model trained from the start to handle multiple data types (like GPT-4V or Gemini). This method is powerful, but requires significant resources.\n\nI broke down each method with practical use cases and example code in my latest blog post:\n\n[Link to blog]\n\nWhat’s the first multimodal project you want to build?"
}