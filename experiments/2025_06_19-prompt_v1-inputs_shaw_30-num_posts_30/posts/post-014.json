{
  "input": "Share video: Fine-tuning LLMs with MLX",
  "writing_steps": "Step 1: Purpose & Target Audience\n- Purpose: Share and promote a new educational video on fine-tuning large language models (LLMs) using MLX, validate the content idea, and encourage fellow learners to engage.\n- Audience: AI enthusiasts, ML engineers, developers, and aspiring practitioners following Shaw, interested in hands-on application of AI tools and techniques.\n\nStep 2: Post Wireframe\n- Hook: Tease a pain point or new opportunity in fine-tuning LLMs, especially with new frameworks like MLX.\n- Meat: Brief summary of what the video covers, why MLX is interesting, personal note on why it matters, and clear link to the video.\n- CTA: Ask if readers have tried MLX or are stuck on any step of LLM fine-tuning.\n\nStep 3: Write the Meat\nFine-tuning LLMs has usually meant wrestling with huge infrastructure or getting locked into certain frameworks. MLX changes that—it’s a new Apple-backed library that’s surprisingly lightweight, and I walk you step-by-step through the process in my latest video.\n\nYou’ll see:\n- How to prep your data\n- A minimal working code example for MLX (with all the gotchas I hit along the way)\n- How to actually get results on your own hardware—not just in the cloud\n\nIf you’ve been curious about modern fine-tuning workflows or want to see MLX in practice, this one’s for you. [Watch here: YOUTUBE_LINK]\n\nStep 4: CTA\nCurious—have you tried MLX for LLM fine-tuning yet?\n\nStep 5A: Explore Hook Angles\n- [Problem/solution] \"Fine-tuning LLMs doesn't have to mean working with clunky, heavyweight tools.\"\n- [Personal story] \"Tried to fine-tune an LLM on my laptop—here’s what actually worked.\"\n- [Results/outcomes] \"How I fine-tuned an LLM using MLX in under an hour.\"\n- [Trend/urgency] \"There’s a new workflow for LLM fine-tuning—and it runs locally.\"\n- [Beginner-to-expert clarity] \"Most tutorials on LLM fine-tuning skip this key step.\"\n\nStep 5B: Notes for Each Angle\n- Problem/solution: Most people stuck with HuggingFace or PyTorch—MLX is a new path.\n- Personal story: My own challenges with setup, MLX was smoothest.\n- Results: Got it running and measurable in under an hour on my own Mac.\n- Trend: MLX is fresh and Apple-backed—it matters for the future of local models.\n- Clarity: A lot of confusion on getting LLMs fine-tuned efficiently on modern hardware.\n\nStep 5C: Choose Hook\nThe problem/solution direction feels sharp for the audience. Final hook:\n\n\"Most people think you need heavyweight tools (and hardware) to fine-tune LLMs. Not anymore.\"\n\nStep 6: Review for Style & Voice\n- Tone is friendly, direct, and grounded in personal experience.\n- Clear bullet points for what’s in the video.\n- Strong single CTA.\n\nStep 7: Final Post\n(see below)\n",
  "final_post": "Most people think you need heavyweight tools (and hardware) to fine-tune LLMs. Not anymore.\n\nFine-tuning LLMs used to mean wrestling with huge infrastructure or getting locked into tricky frameworks. MLX changes that—it’s a new, Apple-backed library that’s lightweight and approachable. In my latest video, I walk you step-by-step through the entire process:\n\n- How to prep your data\n- A minimal, working code example for MLX (including all my gotchas)\n- How to get real results on your own hardware—not just in the cloud\n\nIf you’ve been curious about modern LLM fine-tuning workflows or want to see MLX in practice, jump in: [Watch here: YOUTUBE_LINK]\n\nCurious—have you tried MLX for LLM fine-tuning yet?"
}