{
  "input": "Share blog: multimodal models. 3 ways to make LLMs multimodal.\nLLM + Tools\nLLM + Adapters\nUnified Models",
  "writing_steps": "Step 1: Purpose & Audience\n- Purpose: Educate Shaw's LinkedIn audience and share a recent blog post on multimodal models, laying out the 3 main approaches to make language models (LLMs) multimodal. This also teases deeper content on Shaw's blog.\n- Audience: AI practitioners, technical founders, educators, and curious learners interested in the evolving capabilities of LLMs.\n\nStep 2: Wireframe\n- Hook\n- Brief summary of need for multimodal LLMs\n- Three approaches: LLM + Tools, LLM + Adapters, Unified Models (concise, bullet format)\n- Tease deeper blog content\n- CTA: Ask audience which approach excites them most\n\nStep 3: Body\nLLMs started out as text-only—but the world isn’t just text.\n\nIf you want your AI collaborators to handle images, audio, or even video, you’ll need to make your LLMs multimodal.\n\nHere are three main ways researchers are doing this:\n\n1. **LLM + Tools**\n   - The LLM uses external APIs/tools whenever it needs to process non-text data (think: Google Vision for images, or a speech-to-text tool for audio). Fast to prototype, but the model depends heavily on external services.\n\n2. **LLM + Adapters**\n   - Specialized adapters (lightweight neural networks) convert image, audio, or other inputs into something the LLM can process. These adapters “translate” new modalities into tokens. The model can learn to coordinate with different modalities—but full integration isn’t always seamless.\n\n3. **Unified Multimodal Models**\n   - Some models are trained from scratch to handle text, images, even audio/video, as first-class citizens—think GPT-4V or Google Gemini. Most powerful, but also the hardest and most expensive to build.\n\nI break each of these down (with diagrams and real-world use cases) in my latest blog post.\n\nStep 4: CTA\nWhich approach to multimodal LLMs are you most excited about?\n\nStep 5A: Brainstorm Hook Directions\n- [Problem/solution] “Why can’t LLMs ‘see’ or ‘hear’? Here’s how you fix that.”\n- [Beginner-to-expert clarity] “Most people think LLMs can only handle text. That’s changing fast.”\n- [Trend/urgency] “2024 is the year LLMs go multimodal—3 ways to get there.”\n- [Tease blog] “I just published a breakdown of 3 ways to make your LLMs multimodal.”\n\nStep 5B: Add short notes:\n- [Problem/solution] Most LLMs can’t handle images/audio natively. Solutions? 3 ways below.\n- [Beginner/expert] The notion of ‘text-only’ LLMs is outdated.\n- [Trend] Recent advances (GPT-4V, Gemini) are paving a new path.\n- [Tease blog] Blog link for those wanting a deep dive.\n\nStep 5C: Pick the strongest: [Beginner/expert clarity]: “Most people think LLMs can only handle text. That’s changing fast.”\n\nStep 6: Review for style, sequence, voice\n- The post is concise, educational, and actionable.\n- Concrete, transparent, and high-signal clarity.\n- Ends with only one clear CTA.\n\nStep 7: Write final post.",
  "final_post": "Most people think LLMs can only handle text. That’s changing fast.\n\nThe world is multimodal—so if you want smarter AI collaborators, you need LLMs that can “see,” “hear,” and more.\n\nHere are 3 main ways researchers (and builders) are making LLMs multimodal:\n\n1. **LLM + Tools:** Use external APIs (like image or audio processors) alongside your model. Simple to set up, but relies on third-party tools.\n2. **LLM + Adapters:** Plug in neural adapters to turn images/audio into something LLMs understand. Integrates new data types without retraining the core model.\n3. **Unified Multimodal Models:** Train models (like GPT-4V, Gemini) to natively handle multiple modalities from day one. Most powerful—also most resource-intensive.\n\nI break these down (with visual diagrams and real-world examples) on my blog — check it out for deeper dives.\n\nWhich approach to multimodal LLMs are you most excited about?"
}